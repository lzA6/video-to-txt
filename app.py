import gradio as gr
import os
import json
import logging
import shutil
from pathlib import Path
from typing import List, Dict, Any, Optional, Iterator, Tuple
import requests
import cv2
import time
import psutil
import threading
import numpy as np
from datetime import datetime
import traceback
from dataclasses import dataclass, field
import io
import base64
import inspect
import subprocess  # æ·»åŠ  FFmpeg æ£€æŸ¥ä¾èµ–

# ==============================================================================
# é˜¶æ®µä¸€ï¼šä¾èµ–å¯¼å…¥ä¸å…¨å±€è®¾ç½®
# ==============================================================================

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)
logger = logging.getLogger(__name__)

# --- ä¾èµ–æ£€æŸ¥ä¸åŠ¨æ€å¯¼å…¥ ---
CORE_MODULES_LOADED = True
NVIDIA_GPU_AVAILABLE = False
ADVANCED_FEATURES_AVAILABLE = False
MEDIAINFO_AVAILABLE = False
FFMPEG_AVAILABLE = False

try:
    import pynvml
    pynvml.nvmlInit()
    NVIDIA_GPU_AVAILABLE = True
    logger.info("âœ… pynvml (NVIDIA GPU support) åŠ è½½æˆåŠŸã€‚")
except (ImportError, pynvml.NVMLError) as e:
    NVIDIA_GPU_AVAILABLE = False
    logger.warning(f"âš ï¸ pynvml åŠ è½½å¤±è´¥ï¼ŒGPUç›‘æ§å°†ä¸å¯ç”¨ã€‚é”™è¯¯: {e}")

try:
    from moviepy.editor import VideoFileClip, AudioFileClip, CompositeVideoClip, ImageClip, concatenate_videoclips, TextClip
    import matplotlib
    matplotlib.use('Agg') # é¿å…åœ¨éGUIç¯å¢ƒä¸‹æŠ¥é”™
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm
    import seaborn as sns
   
    def setup_chinese_font():
        font_dir = Path("fonts")
        font_dir.mkdir(exist_ok=True)
        font_files = list(font_dir.glob('*.ttf')) + list(font_dir.glob('*.otf'))
       
        simhei_path = font_dir / "SimHei.ttf"
        if not simhei_path.exists():
            try:
                logger.info("æ­£åœ¨ä¸‹è½½å¤‡ç”¨ä¸­æ–‡å­—ä½“ SimHei.ttf...")
                # ä½¿ç”¨æ›´å¯é çš„å­—ä½“æº
                font_url = "https://github.com/google/fonts/raw/main/ofl/notosanssc/NotoSansSC-Regular.otf"
                response = requests.get(font_url, timeout=20)
                response.raise_for_status()
                with open(simhei_path, 'wb') as f:
                    f.write(response.content)
                logger.info(f"âœ… å¤‡ç”¨å­—ä½“ä¸‹è½½æˆåŠŸ: {simhei_path}")
                font_files.append(simhei_path)
            except Exception as download_e:
                logger.warning(f"ä¸‹è½½å¤‡ç”¨å­—ä½“å¤±è´¥: {download_e}")
       
        if font_files:
            font_path = font_files[0]
            try:
                fm.fontManager.addfont(str(font_path))
                prop = fm.FontProperties(fname=str(font_path))
                font_name = prop.get_name()
               
                plt.rcParams['font.family'] = 'sans-serif'
                plt.rcParams['font.sans-serif'] = [font_name]
                plt.rcParams['axes.unicode_minus'] = False
               
                fig, ax = plt.subplots()
                ax.text(0.5, 0.5, 'æµ‹è¯•ä¸­æ–‡å­—ä½“', ha='center', va='center')
                plt.close(fig)
                # æ£€æŸ¥å­—ä½“æ˜¯å¦çœŸçš„è¢«è®¾ç½®
                if plt.rcParams['font.sans-serif'][0] == font_name:
                    logger.info(f"âœ… æˆåŠŸä» '{font_path.name}' åŠ è½½å¹¶è®¾ç½®ä¸­æ–‡å­—ä½“: {font_name}")
                    return True
                else:
                    raise RuntimeError("å­—ä½“è®¾ç½®æœªç”Ÿæ•ˆ")
            except Exception as e:
                logger.warning(f"åŠ è½½æœ¬åœ°å­—ä½“ '{font_path.name}' å¤±è´¥: {e}ã€‚å°†å¼ºåˆ¶é‡å»ºç¼“å­˜å¹¶é‡è¯•ã€‚")
                try:
                    cachedir = matplotlib.get_cachedir()
                    if os.path.exists(cachedir):
                        shutil.rmtree(cachedir)
                        logger.info("Matplotlib å­—ä½“ç¼“å­˜å·²æ¸…é™¤ï¼Œå°†è‡ªåŠ¨é‡å»ºã€‚")
                except (FileNotFoundError, PermissionError) as cache_e:
                    logger.warning(f"æ— æ³•æ¸…é™¤Matplotlibç¼“å­˜: {cache_e}ï¼Œç»§ç»­å°è¯•ã€‚")
               
                try:
                    fm._fontManager = fm.FontManager()
                    fm.fontManager.addfont(str(font_path))
                    prop = fm.FontProperties(fname=str(font_path))
                    font_name = prop.get_name()
                    plt.rcParams['font.family'] = 'sans-serif'
                    plt.rcParams['font.sans-serif'] = [font_name]
                    plt.rcParams['axes.unicode_minus'] = False
                    logger.info(f"âœ… é‡å»ºç¼“å­˜åï¼ŒæˆåŠŸè®¾ç½®ä¸­æ–‡å­—ä½“: {font_name}")
                    return True
                except Exception as final_e:
                    logger.error(f"é‡å»ºç¼“å­˜åä»æ— æ³•è®¾ç½®å­—ä½“ '{font_name}': {final_e}")

        logger.warning("æœ¬åœ° 'fonts' ç›®å½•ä¸ºç©ºæˆ–å­—ä½“åŠ è½½å¤±è´¥ã€‚å°†å°è¯•è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿå­—ä½“ã€‚")
        font_candidates = ['Microsoft YaHei', 'SimHei', 'DengXian', 'PingFang SC', 'Heiti SC', 'Arial Unicode MS']
        for font_name in font_candidates:
            try:
                if fm.findfont(font_name, fallback_to_default=False):
                    plt.rcParams['font.sans-serif'] = [font_name]
                    plt.rcParams['axes.unicode_minus'] = False
                    logger.info(f"âœ… å›é€€æˆåŠŸï¼šæ‰¾åˆ°å¹¶è®¾ç½®ç³»ç»Ÿå­—ä½“: {font_name}")
                    return True
            except Exception:
                continue
       
        logger.error("âŒ å­—ä½“è®¾ç½®å¤±è´¥ï¼šæœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æœ¬åœ°æˆ–ç³»ç»Ÿå­—ä½“ã€‚å›¾è¡¨ä¸­æ–‡å°†æ˜¾ç¤ºä¸ºæ–¹æ¡†ã€‚")
        return False
    FONT_LOADED_SUCCESSFULLY = setup_chinese_font()
    ADVANCED_FEATURES_AVAILABLE = True
    logger.info("âœ… moviepy, matplotlib, seaborn åŠ è½½æˆåŠŸã€‚")
except ImportError as e:
    ADVANCED_FEATURES_AVAILABLE = False
    logger.error(f"\n{'='*60}\nâŒ è­¦å‘Š: moviepy, matplotlibæˆ–seabornåŠ è½½å¤±è´¥ã€‚AIæ‘˜è¦è§†é¢‘å’Œç”»è´¨å›¾åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚\nè¯¦ç»†å¯¼å…¥é”™è¯¯: {e}\n{'='*60}\n")

try:
    result = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True, creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0)
    if result.returncode == 0:
        FFMPEG_AVAILABLE = True
        logger.info("âœ… FFmpeg å¯ç”¨ï¼ˆMoviePy å°†æ­£å¸¸å·¥ä½œï¼‰ã€‚")
    else:
        FFMPEG_AVAILABLE = False
        logger.warning("âš ï¸ FFmpeg æœªæ£€æµ‹åˆ°ï¼Œè¯·å®‰è£… FFmpeg å¹¶æ·»åŠ è‡³ PATHã€‚")
except (ImportError, FileNotFoundError):
    FFMPEG_AVAILABLE = False
    logger.warning("âš ï¸ FFmpeg æœªåœ¨ PATH ä¸­æ‰¾åˆ°ã€‚è¯·å®‰è£…å¹¶é…ç½®ã€‚")

try:
    from pymediainfo import MediaInfo
    MEDIAINFO_AVAILABLE = True
    logger.info("âœ… pymediainfo åŠ è½½æˆåŠŸã€‚")
except ImportError:
    MEDIAINFO_AVAILABLE = False
    logger.warning(f"\n{'='*60}\nâš ï¸ è­¦å‘Š: pymediainfoæœªå®‰è£…ã€‚å°†æ— æ³•ç”Ÿæˆä¸“ä¸šçš„è¯¦ç»†å…ƒæ•°æ®JSONã€‚\nè¯·è¿è¡Œ: pip install pymediainfo\n{'='*60}\n")

# --- æ ¸å¿ƒé€»è¾‘ç±»å®šä¹‰ ---
@dataclass
class Frame:
    path: Path
    timestamp: float
    metrics: Dict[str, float] = field(default_factory=dict)

class VideoProcessor:
    def __init__(self, video_path: Path, output_dir: Path):
        self.video_path = video_path
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def extract_keyframes(self, frames_per_minute: int, max_frames: int) -> List[Frame]:
        cap = cv2.VideoCapture(str(self.video_path))
        if not cap.isOpened():
            logger.error(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {self.video_path}")
            return []
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
       
        target_frames = min(max_frames, int(duration / 60 * frames_per_minute))
        if target_frames == 0 and total_frames > 0:
            target_frames = 1
        if target_frames == 0:
            cap.release()
            return []
        frame_indices = np.linspace(0, total_frames - 1, target_frames, dtype=int)
       
        extracted_frames = []
        for i, frame_index in enumerate(frame_indices):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
            ret, frame_data = cap.read()
            if ret:
                timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0
                frame_filename = self.output_dir / f"frame_{i:04d}_{timestamp:.2f}s.jpg"
                cv2.imwrite(str(frame_filename), frame_data)
                frame_metrics = get_frame_metrics(frame_data)
                extracted_frames.append(Frame(path=frame_filename, timestamp=timestamp, metrics=frame_metrics))
                logger.info(f"æå–å…³é”®å¸§ {i+1}/{len(frame_indices)}: æ—¶é—´ {timestamp:.2f}s")
       
        cap.release()
        logger.info(f"ä»è§†é¢‘ä¸­æå–äº† {len(extracted_frames)} å¸§ (ç›®æ ‡æ˜¯ {target_frames})")
        return extracted_frames

@dataclass
class AudioTranscript:
    text: str
    segments: List[Dict]
    language: str

class AudioProcessor:
    def __init__(self):
        self.whisper_model = None

    def extract_audio(self, video_path: Path, output_dir: Path) -> Optional[Path]:
        if not ADVANCED_FEATURES_AVAILABLE:
            logger.warning("Moviepy æœªåŠ è½½ï¼Œæ— æ³•æå–éŸ³é¢‘ã€‚")
            return None
        try:
            logger.info(f"æ­£åœ¨ä» {video_path.name} æå–éŸ³é¢‘...")
            video_clip = VideoFileClip(str(video_path))
            if video_clip.audio is None:
                logger.warning(f"è§†é¢‘ {video_path.name} ä¸åŒ…å«éŸ³è½¨ã€‚")
                video_clip.close()
                return None
            audio_path = output_dir / f"{video_path.stem}.mp3"
            video_clip.audio.write_audiofile(str(audio_path), codec='mp3', logger=None)
            video_clip.close()
            logger.info(f"éŸ³é¢‘æå–æˆåŠŸ: {audio_path}")
            return audio_path
        except Exception as e:
            logger.error(f"æå–éŸ³é¢‘å¤±è´¥: {e}")
            return None

    def transcribe(self, audio_path: Path) -> Optional[AudioTranscript]:
        try:
            import whisper
            if self.whisper_model is None:
                logger.info("æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹ (base)...")
                self.whisper_model = whisper.load_model("base")
           
            logger.info(f"æ­£åœ¨ä½¿ç”¨ Whisper è½¬å½•éŸ³é¢‘: {audio_path.name}")
            result = self.whisper_model.transcribe(str(audio_path), fp16=NVIDIA_GPU_AVAILABLE)
            logger.info("éŸ³é¢‘è½¬å½•å®Œæˆã€‚")
            return AudioTranscript(
                text=result.get("text", ""),
                segments=result.get("segments", []),
                language=result.get("language", "")
            )
        except ImportError:
            logger.error("Whisper æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡ŒéŸ³é¢‘è½¬å½•ã€‚è¯·è¿è¡Œ: pip install openai-whisper")
            return None
        except Exception as e:
            logger.error(f"ä½¿ç”¨ Whisper è½¬å½•å¤±è´¥: {e}")
            return None

class PromptLoader:
    def __init__(self, prompt_dir: Optional[str], prompts_config: List[Dict[str, str]]):
        self.prompts = {}
        self.prompt_dir = Path(prompt_dir) if prompt_dir else Path("prompts")
       
        for config in prompts_config:
            name = config.get("name")
            path = config.get("path")
            if name and path:
                try:
                    full_path = self.prompt_dir / path
                    with open(full_path, 'r', encoding='utf-8') as f:
                        self.prompts[name] = f.read()
                    logger.info(f"æˆåŠŸåŠ è½½æç¤ºè¯ '{name}' from {full_path}")
                except FileNotFoundError:
                    logger.error(f"æç¤ºè¯æ–‡ä»¶æœªæ‰¾åˆ°: {full_path}")
                except Exception as e:
                    logger.error(f"åŠ è½½æç¤ºè¯ '{name}' å¤±è´¥: {e}")

    def get_prompt(self, name: str) -> Optional[str]:
        return self.prompts.get(name)

class BaseAPIClient:
    def _encode_image_to_base64(self, image_path: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        except Exception as e:
            logger.error(f"æ— æ³•å°†å›¾ç‰‡ç¼–ç ä¸º Base64: {image_path}, é”™è¯¯: {e}")
            raise
   
    def chat_stream(self, model: str, prompt: str, image_paths: Optional[List[str]] = None, temperature: float = 0.2, timeout: int = 600) -> Iterator[str]:
        raise NotImplementedError

class OllamaClient(BaseAPIClient):
    def __init__(self, url: str = "http://localhost:11434"):
        self.url = url.rstrip('/')
        self.chat_endpoint = f"{self.url}/api/chat"
        logger.info(f"Ollama å®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œå°†ä½¿ç”¨åŸç”ŸèŠå¤©æ¥å£: {self.chat_endpoint}")

    def chat_stream(self, model: str, prompt: str, image_paths: Optional[List[str]] = None, temperature: float = 0.2, timeout: int = 600) -> Iterator[str]:
        headers = {"Content-Type": "application/json"}
        messages = [{"role": "user", "content": prompt}]
       
        if image_paths:
            try:
                encoded_images = [self._encode_image_to_base64(p) for p in image_paths]
                messages[0]["images"] = encoded_images
            except Exception as e:
                yield json.dumps({"error": f"å›¾ç‰‡ç¼–ç å¤±è´¥: {e}"}) + "\n"
                return
        payload = {
            "model": model,
            "messages": messages,
            "stream": True,
            "options": {"temperature": temperature}
        }
        try:
            with requests.post(self.chat_endpoint, headers=headers, data=json.dumps(payload), stream=True, timeout=timeout) as response:
                response.raise_for_status()
                for line in response.iter_lines():
                    if line:
                        yield line.decode('utf-8')
        except requests.exceptions.RequestException as e:
            logger.error(f"è¯·æ±‚ Ollama åŸç”Ÿ API å¤±è´¥: {e}")
            yield json.dumps({"error": f"è¯·æ±‚ Ollama åŸç”Ÿ API å¤±è´¥: {e}"}) + "\n"

class GenericOpenAIAPIClient(BaseAPIClient):
    def __init__(self, api_key: str, api_url: str):
        self.api_key = api_key
        self.api_url = api_url.rstrip('/')
        self.chat_endpoint = f"{self.api_url}/chat/completions"
        logger.info(f"OpenAI å…¼å®¹å®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œå°†ä½¿ç”¨æ¥å£: {self.chat_endpoint}")

    def chat_stream(self, model: str, prompt: str, image_paths: Optional[List[str]] = None, temperature: float = 0.2, timeout: int = 600) -> Iterator[str]:
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
       
        content_parts = [{"type": "text", "text": prompt}]
        if image_paths:
            try:
                for image_path in image_paths:
                    base64_image = self._encode_image_to_base64(image_path)
                    content_parts.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}
                    })
            except Exception as e:
                yield f'data: {json.dumps({"error": f"å›¾ç‰‡ç¼–ç å¤±è´¥: {e}"})}\n\n'
                return
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": content_parts}],
            "temperature": temperature,
            "stream": True
        }
        try:
            with requests.post(self.chat_endpoint, headers=headers, json=payload, stream=True, timeout=timeout) as response:
                response.raise_for_status()
                for line in response.iter_lines():
                    if line.startswith(b'data: '):
                        yield line.decode('utf-8')
        except requests.exceptions.RequestException as e:
            logger.error(f"è¯·æ±‚ OpenAI å…¼å®¹ API å¤±è´¥: {e}")
            yield f'data: {json.dumps({"error": f"è¯·æ±‚ OpenAI å…¼å®¹ API å¤±è´¥: {e}"})}\n\n'

class VideoAnalyzer:
    def __init__(self, client: BaseAPIClient, model: str, prompt_loader: PromptLoader, temperature: float = 0.2, request_timeout: int = 600):
        self.client = client
        self.model = model
        self.prompt_loader = prompt_loader
        self.temperature = temperature
        self.request_timeout = request_timeout
        self.user_prompt = ""
        self.context_length = 4096

    def _process_stream(self, stream_iterator: Iterator[str]) -> Iterator[str]:
        full_response_text = ""
        for chunk_str in stream_iterator:
            if analysis_state.stop_requested:
                break
            try:
                if chunk_str.startswith('data: '):
                    chunk_str = chunk_str[6:]
                    if chunk_str.strip() == '[DONE]':
                        break
                    chunk = json.loads(chunk_str)
                    delta = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
                else:
                    chunk = json.loads(chunk_str)
                    if chunk.get("done"):
                        break
                    delta = chunk.get("message", {}).get("content", "")
               
                if delta:
                    full_response_text += delta
                    yield delta
            except (json.JSONDecodeError, IndexError):
                continue
        yield f"__FULL_RESPONSE_END__{full_response_text}"

    def summarize_all_frames_stream(self, frames: List['Frame'], transcript: 'AudioTranscript') -> Iterator[str]:
        prompt_template = self.prompt_loader.get_prompt("Video Summary")
        if not prompt_template:
            yield "é”™è¯¯: æœªæ‰¾åˆ° 'Video Summary' æç¤ºè¯æ¨¡æ¿ã€‚"
            return
        frame_info = "\n".join([f"- å…³é”®å¸§ at {f.timestamp:.2f}s" for f in frames])
        prompt = prompt_template.format(
            user_prompt=self.user_prompt,
            audio_transcript=transcript.text,
            frame_info=frame_info
        )
       
        if len(prompt) > self.context_length * 2.5:
            prompt = prompt[:int(self.context_length * 2.5)] + "\n...[æç¤ºè¯å› è¿‡é•¿è¢«æˆªæ–­]"
        frame_paths = [str(f.path) for f in frames]
        stream_iterator = self.client.chat_stream(
            model=self.model,
            prompt=prompt,
            image_paths=frame_paths,
            temperature=self.temperature,
            timeout=self.request_timeout
        )
        yield from self._process_stream(stream_iterator)

@dataclass
class AnalysisState:
    is_running: bool = False
    stop_requested: bool = False
    status_message: str = "ç­‰å¾…ä¸­..."
   
analysis_state = AnalysisState()
SETTINGS_FILE = Path("ui_settings.json")

class AppState:
    def __init__(self):
        self.analyzer: Optional[VideoAnalyzer] = None
        self.is_loaded: bool = False
        self.system_stats = {"cpu": 0, "ram": 0, "gpu": 0, "vram": 0}
        self.stop_monitoring = threading.Event()

app_state = AppState()

# å…¨å±€UIç»„ä»¶å¼•ç”¨
status_box, client_type_dd, model_select, api_key_txt, api_url_txt, api_model_txt, load_button, unload_button = [None] * 8
output_report, output_metadata_table, metadata_plot, output_gallery, output_summary_video, output_gif, output_metadata_json = [None] * 7
output_summary_clips_gallery, clip_details_accordion, clip_details_md = None, None, None
run_status_html, analysis_progress, start_button, continue_button, stop_button, refresh_summary_button, clear_outputs_button = [None] * 7
frame_details_accordion, frame_details_md = None, None
analysis_cache_state = None
gif_info_md = None

PRESET_PROMPTS = {
    "å†…å®¹æ€»ç»“ä¸è¯„ä¼°": "è¯·è¯¦ç»†æ€»ç»“è¿™ä¸ªè§†é¢‘çš„æ ¸å¿ƒå†…å®¹ã€å…³é”®ä¿¡æ¯ç‚¹å’Œå™äº‹æµç¨‹ã€‚å¹¶ä»è§‚ä¼—çš„è§’åº¦è¯„ä¼°å…¶æ•´ä½“è´¨é‡ã€è¶£å‘³æ€§å’Œä¿¡æ¯ä»·å€¼ã€‚",
    "æŠ€æœ¯è´¨é‡åˆ†æ": "è¯·ä½œä¸ºä¸€åä¸“ä¸šçš„æ‘„å½±å¸ˆå’Œå‰ªè¾‘å¸ˆï¼Œä¸¥æ ¼è¯„ä¼°è¯¥è§†é¢‘çš„æŠ€æœ¯è´¨é‡ï¼ŒåŒ…æ‹¬æ„å›¾ã€ç¯å…‰ã€è‰²å½©ã€ç„¦ç‚¹ã€ç¨³å®šæ€§ã€å‰ªè¾‘èŠ‚å¥å’ŒéŸ³æ•ˆè®¾è®¡ç­‰æ–¹é¢ã€‚è¯·æä¾›å…·ä½“çš„ä¼˜ç‚¹å’Œå¯ä»¥æ”¹è¿›çš„å»ºè®®ã€‚",
    "æƒ…æ„Ÿä¸é£æ ¼è¯†åˆ«": "è¯·åˆ†æè¿™ä¸ªè§†é¢‘æ‰€ä¼ è¾¾çš„ä¸»è¦æƒ…æ„ŸåŸºè°ƒï¼ˆå¦‚æ¬¢ä¹ã€æ‚²ä¼¤ã€æ‚¬ç–‘ã€åŠ±å¿—ç­‰ï¼‰å’Œè§†è§‰é£æ ¼ï¼ˆå¦‚ç”µå½±æ„Ÿã€çºªå½•ç‰‡ã€Vlogã€å¤å¤ç­‰ï¼‰ã€‚å¹¶æŒ‡å‡ºå“ªäº›è§†å¬å…ƒç´ ï¼ˆå¦‚é…ä¹ã€è‰²è°ƒã€é•œå¤´è¯­è¨€ï¼‰å…±åŒä½œç”¨äºè¿™ç§æ„Ÿå—çš„å½¢æˆã€‚",
    "è‡ªå®šä¹‰": ""
}

# ==============================================================================
# é˜¶æ®µäºŒï¼šå‡½æ•°å®šä¹‰åŒº
# ==============================================================================

def monitor_system_stats():
    while not app_state.stop_monitoring.is_set():
        app_state.system_stats['cpu'] = psutil.cpu_percent()
        app_state.system_stats['ram'] = psutil.virtual_memory().percent
        if NVIDIA_GPU_AVAILABLE:
            try:
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                mem = pynvml.nvmlDeviceGetMemoryInfo(handle)
                app_state.system_stats['gpu'], app_state.system_stats['vram'] = util.gpu, (mem.used / mem.total) * 100
            except pynvml.NVMLError:
                app_state.system_stats['gpu'], app_state.system_stats['vram'] = -1, -1
        time.sleep(2)

def update_status_and_sys_info(message: str = "ç­‰å¾…ä»»åŠ¡å¼€å§‹..."):
    stats_html = get_system_stats_html()
    return f"<div style='text-align:center;'>{message}</div>{stats_html}"

def get_system_stats_html() -> str:
    stats = app_state.system_stats
    gpu_html = f"<div class='stat-item'><span class='label'>GPU</span><div class='bar-container'><div class='bar gpu' style='width: {stats.get('gpu', 0):.1f}%;'></div></div><span class='value'>{stats.get('gpu', 0):.1f}%</span></div><div class='stat-item'><span class='label'>VRAM</span><div class='bar-container'><div class='bar vram' style='width: {stats.get('vram', 0):.1f}%;'></div></div><span class='value'>{stats.get('vram', 0):.1f}%</span></div>" if NVIDIA_GPU_AVAILABLE and stats.get('gpu', -1) != -1 else ""
    return f"<div class='stats-container'>{gpu_html}<div class='stat-item'><span class='label'>CPU</span><div class='bar-container'><div class='bar cpu' style='width: {stats.get('cpu', 0):.1f}%;'></div></div><span class='value'>{stats.get('cpu', 0):.1f}%</span></div><div class='stat-item'><span class='label'>RAM</span><div class='bar-container'><div class='bar ram' style='width: {stats.get('ram', 0):.1f}%;'></div></div><span class='value'>{stats.get('ram', 0):.1f}%</span></div></div>"

def get_advanced_video_metrics(video_path: str, num_frames_to_sample=100):
    if not ADVANCED_FEATURES_AVAILABLE: 
        logger.warning("é«˜çº§è§†é¢‘æŒ‡æ ‡ä¸å¯ç”¨ï¼šMoviePy æˆ– Matplotlib æœªåŠ è½½ã€‚")
        return {}, None
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened(): 
        logger.error(f"æ— æ³•æ‰“å¼€è§†é¢‘ç”¨äºé«˜çº§æŒ‡æ ‡åˆ†æ: {video_path}")
        return {}, None
   
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    if total_frames == 0 or fps == 0:
        cap.release()
        return {}, None
    sample_indices = np.linspace(0, total_frames - 1, min(num_frames_to_sample, total_frames), dtype=int)
   
    metrics_over_time = {'timestamps': [], 'brightness': [], 'saturation': [], 'sharpness': []}
    frame_durations = []
    last_timestamp = 0
    for idx in sample_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if not ret: 
            logger.warning(f"è¯»å–é‡‡æ ·å¸§ {idx} å¤±è´¥ï¼Œç»§ç»­...")
            continue
       
        timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0
        metrics_over_time['timestamps'].append(timestamp)
       
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
       
        metrics_over_time['brightness'].append(np.mean(gray))
        metrics_over_time['saturation'].append(np.mean(hsv[:, :, 1]))
        metrics_over_time['sharpness'].append(cv2.Laplacian(gray, cv2.CV_64F).var())
       
        if last_timestamp > 0:
            duration = timestamp - last_timestamp
            if duration > 0: frame_durations.append(1.0 / duration)
        last_timestamp = timestamp
    cap.release()
   
    avg_metrics = {
        "å¹³å‡äº®åº¦ (0-255)": np.mean(metrics_over_time['brightness']) if metrics_over_time['brightness'] else 0,
        "å¹³å‡é¥±å’Œåº¦ (0-255)": np.mean(metrics_over_time['saturation']) if metrics_over_time['saturation'] else 0,
        "å¹³å‡æ¸…æ™°åº¦ (æ‹‰æ™®æ‹‰æ–¯æ–¹å·®)": np.mean(metrics_over_time['sharpness']) if metrics_over_time['sharpness'] else 0
    }
    logger.info(f"é«˜çº§æŒ‡æ ‡è®¡ç®—å®Œæˆ: äº®åº¦={avg_metrics['å¹³å‡äº®åº¦ (0-255)']:.2f}, é¥±å’Œåº¦={avg_metrics['å¹³å‡é¥±å’Œåº¦ (0-255)']:.2f}, æ¸…æ™°åº¦={avg_metrics['å¹³å‡æ¸…æ™°åº¦ (æ‹‰æ™®æ‹‰æ–¯æ–¹å·®)']:.2f}")
    sns.set_style("darkgrid")
    fig, axes = plt.subplots(2, 2, figsize=(15, 8))
    fig.suptitle('è§†é¢‘ç”»è´¨éšæ—¶é—´å˜åŒ–åˆ†æ', fontsize=16)
    sns.lineplot(x='timestamps', y='brightness', data=metrics_over_time, ax=axes[0, 0], color='skyblue', label=f"å¹³å‡å€¼: {avg_metrics['å¹³å‡äº®åº¦ (0-255)']:.2f}")
    axes[0, 0].set_title('äº®åº¦å˜åŒ–'); axes[0, 0].set_xlabel("æ—¶é—´ (ç§’)"); axes[0, 0].set_ylabel("æ•°å€¼"); axes[0, 0].legend()
    sns.lineplot(x='timestamps', y='saturation', data=metrics_over_time, ax=axes[0, 1], color='salmon', label=f"å¹³å‡å€¼: {avg_metrics['å¹³å‡é¥±å’Œåº¦ (0-255)']:.2f}")
    axes[0, 1].set_title('é¥±å’Œåº¦å˜åŒ–'); axes[0, 1].set_xlabel("æ—¶é—´ (ç§’)"); axes[0, 1].set_ylabel("æ•°å€¼"); axes[0, 1].legend()
    sns.lineplot(x='timestamps', y='sharpness', data=metrics_over_time, ax=axes[1, 0], color='lightgreen', label=f"å¹³å‡å€¼: {avg_metrics['å¹³å‡æ¸…æ™°åº¦ (æ‹‰æ™®æ‹‰æ–¯æ–¹å·®)']:.2f}")
    axes[1, 0].set_title('æ¸…æ™°åº¦ (é”åŒ–) å˜åŒ–'); axes[1, 0].set_xlabel("æ—¶é—´ (ç§’)"); axes[1, 0].set_ylabel("æ•°å€¼"); axes[1, 0].legend()
   
    if frame_durations:
        mean_fps = np.mean(frame_durations)
        sns.histplot(frame_durations, ax=axes[1, 1], color='orchid', bins=20, kde=True)
        axes[1, 1].set_title(f'å¸§ç‡ç¨³å®šæ€§ (å¹³å‡: {mean_fps:.2f} FPS)'); axes[1, 1].set_xlabel("å¸§ç‡ (FPS)")
        axes[1, 1].axvline(mean_fps, color='r', linestyle='--', label=f'å¹³å‡å¸§ç‡: {mean_fps:.2f}'); axes[1, 1].legend()
    else:
        axes[1, 1].text(0.5, 0.5, 'æ’å®šå¸§ç‡æˆ–æ— æ³•è®¡ç®—å¸§ç‡å˜åŒ–', ha='center', va='center'); axes[1, 1].set_title('å¸§ç‡ç¨³å®šæ€§')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    return avg_metrics, fig

def get_frame_metrics(frame: np.ndarray) -> dict:
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    return {
        "äº®åº¦ (0-255)": np.mean(gray),
        "å¯¹æ¯”åº¦ (æ ‡å‡†å·®)": np.std(gray),
        "é¥±å’Œåº¦ (0-255)": np.mean(hsv[:, :, 1]),
        "æ¸…æ™°åº¦ (æ‹‰æ™®æ‹‰æ–¯æ–¹å·®)": cv2.Laplacian(gray, cv2.CV_64F).var()
    }

def create_summary_media_artifacts(
    original_video_path: str,
    video_duration: float,
    frames: List[Frame],
    output_dir: Path,
    video_stem: str,
    num_clips: int,
    clip_duration_around_keyframe: float,
    make_video: bool,
    make_gif: bool,
    gif_resolution: str
) -> Tuple[Optional[List[str]], Optional[List[Frame]], Optional[str], Optional[str]]:
    """
    åˆ›å»ºåŠ¨æ€è§†é¢‘ç‰‡æ®µæ‘˜è¦ã€æ‹¼æ¥è§†é¢‘å’ŒGIFã€‚
    è¿”å›: (ç‰‡æ®µè·¯å¾„åˆ—è¡¨, é€‰ä¸­çš„å¸§åˆ—è¡¨, æ‹¼æ¥è§†é¢‘è·¯å¾„, GIFè·¯å¾„)
    """
    logger.info(f"å¼€å§‹åˆ›å»ºæ‘˜è¦åª’ä½“... å¯ç”¨è§†é¢‘: {make_video}, å¯ç”¨GIF: {make_gif}, ç‰‡æ®µæ•°: {num_clips}, æ—¶é•¿: {clip_duration_around_keyframe}s, åˆ†è¾¨ç‡: {gif_resolution}")
    if not ADVANCED_FEATURES_AVAILABLE or not (make_video or make_gif):
        logger.warning(f"è·³è¿‡æ‘˜è¦åª’ä½“åˆ›å»ºï¼Œå› ä¸ºä¾èµ–é¡¹ä¸å¯ç”¨(ADVANCED_FEATURES_AVAILABLE={ADVANCED_FEATURES_AVAILABLE})æˆ–ç”¨æˆ·æœªå¯ç”¨ã€‚")
        if not FFMPEG_AVAILABLE:
            logger.error("FFMPEG æœªå¯ç”¨ï¼Œè¿™æ˜¯ MoviePy å¤±è´¥çš„ä¸»è¦åŸå› ã€‚è¯·å®‰è£… FFmpegã€‚")
            gr.Warning("ç”Ÿæˆæ‘˜è¦åª’ä½“å¤±è´¥ï¼FFmpeg æœªå®‰è£…æˆ–æœªåœ¨ PATH ä¸­ã€‚è¯·ä¸‹è½½ FFmpeg å¹¶é…ç½®ç¯å¢ƒå˜é‡ã€‚")
        return None, None, None, None
   
    if not frames:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å¸§ï¼Œæ— æ³•åˆ›å»ºæ‘˜è¦åª’ä½“ã€‚")
        return None, None, None, None
    
    logger.info(f"æ­£åœ¨ä» {len(frames)} ä¸ªå€™é€‰å¸§ä¸­é€‰æ‹© {num_clips} ä¸ªæœ€æ¸…æ™°çš„å¸§...")
    # ç¡®ä¿ num_clips ä¸è¶…è¿‡å¯ç”¨å¸§æ•°
    num_clips = min(num_clips, len(frames))
    sorted_frames = sorted(frames, key=lambda x: x.metrics.get('æ¸…æ™°åº¦ (æ‹‰æ™®æ‹‰æ–¯æ–¹å·®)', 0), reverse=True)
    selected_frames = sorted(sorted_frames[:int(num_clips)], key=lambda x: x.timestamp)
    if not selected_frames:
        logger.warning("æ ¹æ®æ¸…æ™°åº¦æ’åºåï¼Œæ²¡æœ‰å¯é€‰çš„å¸§æ¥åˆ›å»ºæ‘˜è¦åª’ä½“ã€‚")
        return None, None, None, None
    logger.info(f"å·²é€‰å®š {len(selected_frames)} ä¸ªå¸§ç”¨äºç”Ÿæˆç‰‡æ®µã€‚æ—¶é—´ç‚¹: {[f.timestamp for f in selected_frames]}")
    
    individual_clip_paths = []
    
    logger.info(f"æ­£åœ¨ä»åŸå§‹è§†é¢‘ä¸­æˆªå– {len(selected_frames)} ä¸ªåŠ¨æ€ç‰‡æ®µ...")
    success_count = 0
    try:
        # ä½¿ç”¨ 'with' è¯­å¥ç¡®ä¿ä¸»è§†é¢‘æ–‡ä»¶åœ¨å¤„ç†å®Œåè¢«å…³é—­
        with VideoFileClip(original_video_path) as video:
            logger.info(f"è§†é¢‘åŠ è½½æˆåŠŸï¼Œæ—¶é•¿: {video.duration}s")
            for i, frame_obj in enumerate(selected_frames):
                try:
                    start_time = max(0, frame_obj.timestamp - clip_duration_around_keyframe / 2)
                    end_time = min(video.duration, frame_obj.timestamp + clip_duration_around_keyframe / 2)
                   
                    logger.info(f"ç‰‡æ®µ {i+1}/{len(selected_frames)}: æˆªå–æ—¶é—´ä» {start_time:.2f}s åˆ° {end_time:.2f}sã€‚")
                    if end_time <= start_time:
                        logger.warning(f"è·³è¿‡ç‰‡æ®µ {i}ï¼Œå› ä¸ºè®¡ç®—å‡ºçš„ç»“æŸæ—¶é—´({end_time})æ—©äºæˆ–ç­‰äºå¼€å§‹æ—¶é—´({start_time})ã€‚")
                        continue
                    
                    # ä»ä¸»è§†é¢‘ä¸­æˆªå–å­å‰ªè¾‘
                    sub_clip = video.subclip(start_time, end_time)
                   
                    clip_path = get_unique_filepath(output_dir, f"{video_stem}_clip_{i:02d}.mp4")
                    
                    # å°†å­å‰ªè¾‘å†™å…¥ç‹¬ç«‹çš„MP4æ–‡ä»¶ï¼ŒåŒ…å«éŸ³é¢‘
                    sub_clip.write_videofile(str(clip_path), codec='libx264', audio_codec='aac', logger=None, threads=4)
                   
                    individual_clip_paths.append(str(clip_path))
                    success_count += 1
                    logger.info(f"âœ… æˆåŠŸåˆ›å»ºè§†é¢‘ç‰‡æ®µ {i+1}/{len(selected_frames)}: {clip_path.name} (å¤§å°: {os.path.getsize(clip_path)/1024:.1f} KB)")
                    
                    # ç«‹å³å…³é—­å­å‰ªè¾‘ä»¥é‡Šæ”¾èµ„æº
                    sub_clip.close()

                except Exception as e:
                    logger.error(f"âŒ åˆ›å»ºè§†é¢‘ç‰‡æ®µ {i} (æ—¶é—´ç‚¹: {frame_obj.timestamp:.2f}s) æ—¶å‡ºé”™: {e}", exc_info=True)
                    gr.Warning(f"åˆ›å»ºç‰‡æ®µ {i+1} å¤±è´¥: {e}ã€‚ç»§ç»­ç”Ÿæˆå…¶ä»–ç‰‡æ®µã€‚")
                    continue
            logger.info(f"ç‰‡æ®µç”Ÿæˆå®Œæˆ: {success_count}/{len(selected_frames)} æˆåŠŸã€‚")
    except OSError as e:
        logger.error(f"âŒ MoviePy ä¸¥é‡é”™è¯¯: æ— æ³•å¤„ç†è§†é¢‘æ–‡ä»¶ã€‚è¿™é€šå¸¸æ„å‘³ç€ FFmpeg æœªå®‰è£…æˆ–æœªåœ¨ç³»ç»Ÿè·¯å¾„ä¸­ã€‚é”™è¯¯: {e}", exc_info=True)
        gr.Warning("ç”Ÿæˆè§†é¢‘æ‘˜è¦å¤±è´¥ï¼è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£… FFmpeg å¹¶å°†å…¶æ·»åŠ è‡³ç³»ç»Ÿç¯å¢ƒå˜é‡(PATH)ã€‚")
        return individual_clip_paths, selected_frames, None, None
   
    if not individual_clip_paths:
        logger.warning("âš ï¸ æœªèƒ½æˆåŠŸåˆ›å»ºä»»ä½•è§†é¢‘ç‰‡æ®µï¼Œæ— æ³•è¿›è¡Œæ‹¼æ¥ã€‚")
        return individual_clip_paths, selected_frames, None, None

    # --- æ‹¼æ¥é˜¶æ®µ ---
    # ã€å…³é”®ä¿®å¤ã€‘: ä»ç£ç›˜é‡æ–°åŠ è½½æ‰€æœ‰ç‰‡æ®µæ–‡ä»¶è¿›è¡Œæ‹¼æ¥ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å†…å­˜ä¸­å¯èƒ½å·²å¤±æ•ˆçš„subclipå¯¹è±¡
    concatenated_video_path, gif_path = None, None
    reloaded_clips = []
    try:
        logger.info(f"æ­£åœ¨ä» {len(individual_clip_paths)} ä¸ªå·²ä¿å­˜çš„ç‰‡æ®µæ–‡ä»¶é‡æ–°åŠ è½½ä»¥è¿›è¡Œæ‹¼æ¥...")
        reloaded_clips = [VideoFileClip(p) for p in individual_clip_paths]
        
        if not reloaded_clips:
            raise ValueError("é‡æ–°åŠ è½½ç‰‡æ®µæ–‡ä»¶ååˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•æ‹¼æ¥ã€‚")

        final_clip = concatenate_videoclips(reloaded_clips, method="compose")
        logger.info("ç‰‡æ®µæ‹¼æ¥æˆåŠŸï¼Œå¼€å§‹å¯¼å‡º...")
       
        if make_video:
            concatenated_video_path = get_unique_filepath(output_dir, f"{video_stem}_summary_concatenated.mp4")
            logger.info(f"æ­£åœ¨ç”Ÿæˆæ‹¼æ¥æ‘˜è¦è§†é¢‘: {concatenated_video_path}")
            final_clip.write_videofile(str(concatenated_video_path), fps=24, codec='libx264', audio_codec='aac', logger=None, threads=4)
            logger.info(f"âœ… æ‹¼æ¥æ‘˜è¦è§†é¢‘ç”ŸæˆæˆåŠŸ (å¤§å°: {os.path.getsize(concatenated_video_path)/1024/1024:.1f} MB)")
       
        if make_gif:
            gif_path = get_unique_filepath(output_dir, f"{video_stem}_summary.gif")
            resolution_map = {"ä½": 0.3, "ä¸­": 0.5, "é«˜": 0.8}
            resized_clip = final_clip.resize(resolution_map.get(gif_resolution, 0.5))
            logger.info(f"æ­£åœ¨ç”Ÿæˆæ‘˜è¦GIF (åˆ†è¾¨ç‡: {gif_resolution}): {gif_path}")
            resized_clip.write_gif(str(gif_path), fps=10, logger=None)
            logger.info(f"âœ… æ‘˜è¦GIFç”ŸæˆæˆåŠŸ (å¤§å°: {os.path.getsize(gif_path)/1024:.1f} KB)")
            resized_clip.close()
        
        # å…³é—­æœ€ç»ˆçš„åˆæˆå‰ªè¾‘
        final_clip.close()
           
    except Exception as e:
        logger.error(f"âŒ æ‹¼æ¥è§†é¢‘æˆ–ç”ŸæˆGIFæ—¶å‡ºé”™: {e}", exc_info=True)
        gr.Warning(f"æ‹¼æ¥è§†é¢‘æˆ–ç”ŸæˆGIFæ—¶å‡ºé”™: {e}")
    finally:
        # ã€é‡è¦ã€‘: ç¡®ä¿æ‰€æœ‰é‡æ–°åŠ è½½çš„å‰ªè¾‘éƒ½è¢«å…³é—­ï¼Œä»¥é‡Šæ”¾æ–‡ä»¶å¥æŸ„
        logger.info("æ­£åœ¨å…³é—­æ‰€æœ‰ç”¨äºæ‹¼æ¥çš„è§†é¢‘ç‰‡æ®µèµ„æº...")
        for clip in reloaded_clips:
            try:
                clip.close()
            except Exception as close_err:
                logger.warning(f"å…³é—­ä¸€ä¸ªä¸´æ—¶ç‰‡æ®µæ—¶å‡ºé”™: {close_err}")
        logger.info("æ‰€æœ‰ä¸´æ—¶ç‰‡æ®µèµ„æºå·²å…³é—­ã€‚")
    
    logger.info("æ‘˜è¦åª’ä½“åˆ›å»ºæµç¨‹ç»“æŸã€‚")
    return individual_clip_paths, selected_frames, str(concatenated_video_path) if concatenated_video_path else None, str(gif_path) if gif_path else None

def get_unique_filepath(output_dir: Path, filename: str) -> Path:
    output_dir.mkdir(parents=True, exist_ok=True)
    base, ext = os.path.splitext(filename)
    filepath = output_dir / filename
    if filepath.exists():
        timestamp = datetime.now().strftime("_%Y%m%d%H%M%S")
        filepath = output_dir / f"{base}{timestamp}{ext}"
    return filepath

def get_video_metadata(video_path: str) -> (dict, float):
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened(): 
            logger.error(f"æ— æ³•æ‰“å¼€è§†é¢‘è·å–å…ƒæ•°æ®: {video_path}")
            return {}, 0
        width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps, frame_count = cap.get(cv2.CAP_PROP_FPS), int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = frame_count / fps if fps > 0 else 0
        fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))
        codec = fourcc.to_bytes(4, 'little').decode('utf-8', errors='ignore').strip('\x00')
        cap.release()
        file_size_mb = os.path.getsize(video_path) / (1024 * 1024)
        meta = {"æ–‡ä»¶å": os.path.basename(video_path), "æ–‡ä»¶å¤§å° (MB)": f"{file_size_mb:.2f}", "æ—¶é•¿ (ç§’)": f"{duration:.2f}", "åˆ†è¾¨ç‡": f"{width}x{height}", "å¸§ç‡": f"{fps:.2f}", "æ€»å¸§æ•°": frame_count, "ç¼–ç æ ¼å¼": codec or "æœªçŸ¥"}
        logger.info(f"åŸºæœ¬å…ƒæ•°æ®æå–æˆåŠŸ: {meta}")
        return meta, duration
    except Exception as e:
        logger.error(f"æå–å…ƒæ•°æ®å¤±è´¥: {e}")
        return {}, 0

def detect_ollama_models(url: str = "http://localhost:11434") -> List[str]:
    try:
        response = requests.get(f"{url.rstrip('/')}/api/tags", timeout=3)
        response.raise_for_status()
        return sorted([model["name"] for model in response.json().get("models", [])])
    except requests.exceptions.RequestException:
        return []

def refresh_models_action():
    logger.info("UIæ“ä½œï¼šåˆ·æ–°å¯ç”¨Ollamaæ¨¡å‹åˆ—è¡¨")
    models = detect_ollama_models()
    return gr.update(choices=models, value=models[0] if models else None)

def get_ollama_status():
    status_text, running_models_data, running_model_names = "", [], []
    try:
        response = requests.get("http://localhost:11434/", timeout=3)
        response.raise_for_status()
        ps_response = requests.get("http://localhost:11434/api/ps", timeout=3)
        ps_response.raise_for_status()
        models_info = ps_response.json().get("models", [])
        status_text += "âœ… **Ollama æœåŠ¡åœ¨çº¿**\n\n" + ("å½“å‰æ²¡æœ‰æ¨¡å‹åŠ è½½åˆ°å†…å­˜ä¸­ã€‚\n\n" if not models_info else "")
        for model in models_info:
            running_model_names.append(model['name'])
            running_models_data.append([model['name'], f"{model['size'] / 1e9:.2f} GB"])
        if NVIDIA_GPU_AVAILABLE:
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            total_vram_gb, used_vram_gb = mem_info.total / 1e9, mem_info.used / 1e9
            status_text += f"**GPUçŠ¶æ€**: NVIDIA GPU | æ€»æ˜¾å­˜: {total_vram_gb:.2f} GB | å·²ç”¨: {used_vram_gb:.2f} GB\n\n"
            if total_vram_gb < 20: status_text += "<p style='color:orange;'>âš ï¸ **è­¦å‘Š**: æ‚¨çš„æ€»æ˜¾å­˜ä½äº20GBï¼ŒOllamaå¯èƒ½å·²è¿›å…¥ä½æ˜¾å­˜æ¨¡å¼ï¼Œæ€§èƒ½ä¼šå—å½±å“ã€‚</p>"
    except requests.exceptions.RequestException:
        status_text = "<p style='color:red;'>âŒ **é”™è¯¯**: Ollama æœåŠ¡æœªè¿è¡Œæˆ–æ— æ³•è®¿é—®ã€‚è¯·å…ˆåœ¨ç»ˆç«¯å¯åŠ¨OllamaæœåŠ¡ã€‚</p>"
    except Exception as e:
        status_text = f"<p style='color:red;'>âŒ **é”™è¯¯**: è·å–OllamaçŠ¶æ€æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}</p>"
    return status_text, running_models_data, gr.update(choices=running_model_names, interactive=True)

def unload_ollama_model(model_to_unload):
    logger.info(f"UIæ“ä½œï¼šå°è¯•ä»å†…å­˜å¸è½½æ¨¡å‹ '{model_to_unload}'")
    if not model_to_unload:
        gr.Warning("è¯·ä»ä¸‹æ‹‰èœå•ä¸­é€‰æ‹©ä¸€ä¸ªè¦å¸è½½çš„æ¨¡å‹ã€‚")
        return get_ollama_status()
    try:
        response = requests.post("http://localhost:11434/api/unload", json={"name": model_to_unload}, timeout=10)
        if response.status_code == 404: gr.Warning(f"æ¨¡å‹ '{model_to_unload}' æœªæ‰¾åˆ°ï¼Œå¯èƒ½å·²è¢«å¸è½½ã€‚")
        response.raise_for_status()
        time.sleep(2)
        gr.Info(f"âœ… æ¨¡å‹ '{model_to_unload}' å·²æˆåŠŸä»å†…å­˜ä¸­å¸è½½ã€‚")
    except requests.exceptions.RequestException as e:
        gr.Error(f"å¸è½½æ¨¡å‹æ—¶è¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        gr.Error(f"å¸è½½æ¨¡å‹æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    return get_ollama_status()

def detect_and_set_context(model_name):
    logger.info(f"UIæ“ä½œï¼šæ£€æµ‹æ¨¡å‹ '{model_name}' çš„æ¨èä¸Šä¸‹æ–‡é•¿åº¦")
    if not model_name:
        gr.Warning("è¯·å…ˆåœ¨ä¸Šæ–¹é€‰æ‹©ä¸€ä¸ªOllamaæ¨¡å‹ã€‚")
        return 2048
    try:
        response = requests.post("http://localhost:11434/api/show", json={"name": model_name}, timeout=10)
        response.raise_for_status()
        details = response.json()
        parameters_str = details.get("parameters", "")
        for line in parameters_str.split('\n'):
            if line.startswith("num_ctx"):
                try:
                    context_size = int(line.split()[1])
                    gr.Info(f"âœ… å·²è®¾ç½®ä¸ºæ¨¡å‹æ¨èçš„æœ€å¤§ä¸Šä¸‹æ–‡: {context_size}")
                    return context_size
                except (ValueError, IndexError): continue
        gr.Warning("âš ï¸ æœªèƒ½è‡ªåŠ¨æ£€æµ‹åˆ°ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå·²è®¾ä¸ºé»˜è®¤å€¼4096ã€‚")
        return 4096
    except Exception as e:
        gr.Error(f"âŒ æ£€æµ‹å¤±è´¥: {e}")
        return 2048

def load_model_action(client_type, ollama_model, api_key, api_url, api_model):
    logger.info(f"UIæ“ä½œï¼šå¼€å§‹åŠ è½½æ¨¡å‹ã€‚ç±»å‹: {client_type}, Ollamaæ¨¡å‹: {ollama_model}, APIæ¨¡å‹: {api_model}")
    try:
        if client_type == "Ollama":
            model_name_to_load = ollama_model
            if not model_name_to_load: raise ValueError("Ollamaæ¨¡å‹åç§°ä¸èƒ½ä¸ºç©ºã€‚")
            client = OllamaClient()
        elif client_type == "OpenAI-compatible API":
            model_name_to_load = api_model
            if not api_url: raise ValueError("API URL ä¸èƒ½ä¸ºç©ºã€‚")
            client = GenericOpenAIAPIClient(api_key=api_key, api_url=api_url)
        else:
            raise ValueError(f"æœªçŸ¥çš„å®¢æˆ·ç«¯ç±»å‹: {client_type}")
        prompts_config = [{"name": "Video Summary", "path": "frame_analysis/video_summary.txt"}]
        prompt_loader = PromptLoader(prompt_dir="prompts", prompts_config=prompts_config)
        app_state.analyzer = VideoAnalyzer(client, model_name_to_load, prompt_loader)
        app_state.is_loaded = True
       
        gr.Info(f"âœ… å®¢æˆ·ç«¯ '{client_type}' åŠ è½½æ¨¡å‹ '{model_name_to_load}' æˆåŠŸï¼")
        return f"åŠ è½½æˆåŠŸ: {model_name_to_load}", gr.update(interactive=False), gr.update(interactive=True)
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½æ—¶å‡ºé”™: {e}", exc_info=True)
        app_state.is_loaded = False
        gr.Error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return f"é”™è¯¯: {e}", gr.update(interactive=True), gr.update(interactive=False)

def unload_model_action():
    logger.info("UIæ“ä½œï¼šå¸è½½å½“å‰åº”ç”¨å†…æ¨¡å‹")
    if app_state.is_loaded and isinstance(app_state.analyzer.client, OllamaClient):
        model_to_unload = app_state.analyzer.model
        logger.info(f"æ£€æµ‹åˆ°Ollamaå®¢æˆ·ç«¯ï¼Œå°è¯•ä»å†…å­˜å¸è½½æ¨¡å‹ '{model_to_unload}'")
        try:
            response = requests.post("http://localhost:11434/api/unload", json={"name": model_to_unload}, timeout=10)
            if response.status_code == 200: gr.Info(f"å·²å‘Ollamaå‘é€å¸è½½ '{model_to_unload}' çš„è¯·æ±‚ã€‚")
            elif response.status_code != 404: gr.Warning(f"å‘Ollamaå‘é€å¸è½½è¯·æ±‚å¤±è´¥: {response.text}")
        except Exception as e:
            gr.Warning(f"è¿æ¥Ollamaå¸è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
    app_state.is_loaded = False
    app_state.analyzer = None
    gr.Info("âœ… åº”ç”¨å†…æ¨¡å‹åŠèµ„æºå·²é‡Šæ”¾ã€‚")
    return "æ¨¡å‹å·²å¸è½½", gr.update(interactive=True), gr.update(interactive=False)

def clear_all_outputs_action():
    gr.Info("å·²æ¸…ç©ºæ‰€æœ‰è¾“å‡ºå†…å®¹ã€‚")
    return (
        update_status_and_sys_info("ç­‰å¾…ä»»åŠ¡å¼€å§‹..."),
        None, # output_report
        gr.update(value=None, visible=False), # output_metadata_table
        gr.update(value=None, visible=False), # metadata_plot
        gr.update(value=None, visible=False), # output_gallery
        gr.update(value=None, visible=False), # output_summary_video
        gr.update(value=None, visible=False), # output_gif
        gr.update(value=None, visible=False), # gif_info_md
        gr.update(value=None, visible=False), # output_metadata_json
        gr.update(value=None, visible=False), # output_summary_clips_gallery
        gr.update(visible=False), # clip_details_accordion
        None, # clip_details_md
        gr.update(visible=False), # frame_details_accordion
        None, # frame_details_md
        None, # analysis_cache_state
        gr.update(visible=True, interactive=True), # start_button
        gr.update(visible=False), # continue_button
        gr.update(visible=False), # stop_button
        gr.update(interactive=False), # refresh_summary_button
    )

# --- æ ¸å¿ƒåˆ†æå‡½æ•° ---

def phase_1_extraction(
    video_file: str, enable_audio: bool, frames_per_min: int, max_frames: int,
    output_save_path: str,
    enable_summary_video: bool, enable_gif: bool, summary_clips: int, summary_duration: float, gif_resolution: str,
    progress=gr.Progress(track_tqdm=True)
):
    if not video_file:
        gr.Error("è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ï¼")
        yield {
            run_status_html: update_status_and_sys_info("âŒ é”™è¯¯: è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ï¼"),
            start_button: gr.update(interactive=True),
            stop_button: gr.update(interactive=False)
        }
        return
    
    global analysis_state
    analysis_state = AnalysisState(is_running=True)
   
    yield {
        run_status_html: update_status_and_sys_info("ğŸš€ é˜¶æ®µ 1: å¼€å§‹æ•°æ®æ•´ç†..."),
        start_button: gr.update(interactive=False),
        stop_button: gr.update(interactive=True),
        continue_button: gr.update(visible=False),
        refresh_summary_button: gr.update(visible=False),
        output_report: "", output_metadata_table: gr.update(visible=False),
        metadata_plot: gr.update(visible=False), output_gallery: gr.update(visible=False),
        output_metadata_json: gr.update(visible=False), output_summary_video: gr.update(visible=False),
        output_gif: gr.update(visible=False), gif_info_md: gr.update(visible=False),
        frame_details_accordion: gr.update(visible=False),
        output_summary_clips_gallery: gr.update(visible=False), clip_details_accordion: gr.update(visible=False)
    }
    
    output_dir = Path(output_save_path) if output_save_path else Path("gradio_output")
    video_path = Path(video_file)
    video_output_dir = output_dir / video_path.stem
   
    cache = {"video_path": str(video_path), "output_dir": str(video_output_dir), "frames": [], "transcript": None, "metadata": {}, "plot": None, "media_info_json": {}, "video_duration": 0}
    
    try:
        # 1. æå–å…³é”®å¸§
        progress(0.1, desc="æå–å…³é”®å¸§...")
        yield {run_status_html: update_status_and_sys_info(f"å¤„ç†è§†é¢‘: {video_path.name}<br>é˜¶æ®µ 1: æå–å…³é”®å¸§...")}
        frame_processor = VideoProcessor(video_path, video_output_dir / "frames")
        frames = frame_processor.extract_keyframes(frames_per_minute=frames_per_min, max_frames=max_frames)
        if not frames:
            raise ValueError("æœªèƒ½ä»è§†é¢‘ä¸­æå–ä»»ä½•å…³é”®å¸§ã€‚")
        cache["frames"] = frames
        gallery_items = [(str(f.path), f"æ—¶é—´: {f.timestamp:.2f}s") for f in frames]
        yield {output_gallery: gr.update(value=gallery_items, visible=True)}
        gr.Info("âœ… å…³é”®å¸§ç”»å»Šå·²ç”Ÿæˆï¼")

        # 2. åˆ†æå…ƒæ•°æ®ä¸ç”»è´¨
        progress(0.3, desc="åˆ†æå…ƒæ•°æ®ä¸ç”»è´¨...")
        yield {run_status_html: update_status_and_sys_info(f"å¤„ç†è§†é¢‘: {video_path.name}<br>é˜¶æ®µ 1: åˆ†æå…ƒæ•°æ®ä¸ç”»è´¨...")}
        basic_meta, duration = get_video_metadata(str(video_path))
        cache["video_duration"] = duration
        adv_metrics, plot = get_advanced_video_metrics(str(video_path))
        meta_md = f"### ğŸ“Š è§†é¢‘å…ƒæ•°æ®: {video_path.name}\n\n| å‚æ•° | å€¼ |\n|---|---|\n"
        for k, v in basic_meta.items(): meta_md += f"| {k} | {v} |\n"
        for k, v in adv_metrics.items(): meta_md += f"| {k} | {f'{v:.2f}' if isinstance(v, float) else v} |\n"
        cache["metadata"] = meta_md
        cache["plot"] = plot
        yield {output_metadata_table: gr.update(value=meta_md, visible=True), metadata_plot: gr.update(value=plot, visible=True)}
        if MEDIAINFO_AVAILABLE:
            media_info_json = MediaInfo.parse(str(video_path)).to_data()
            cache["media_info_json"] = media_info_json
            yield {output_metadata_json: gr.update(value={"media_info": media_info_json}, visible=True)}

        # 3. å¤„ç†éŸ³é¢‘
        transcript_obj = AudioTranscript(text="ï¼ˆéŸ³é¢‘åˆ†æå·²ç¦ç”¨ï¼‰", segments=[], language="")
        if enable_audio:
            progress(0.5, desc="æå–å¹¶è½¬å½•éŸ³é¢‘...")
            yield {run_status_html: update_status_and_sys_info(f"å¤„ç†è§†é¢‘: {video_path.name}<br>é˜¶æ®µ 1: å¤„ç†éŸ³é¢‘...")}
            audio_processor = AudioProcessor()
            audio_file_path = audio_processor.extract_audio(video_path, video_output_dir)
            if audio_file_path:
                transcript_obj = audio_processor.transcribe(audio_file_path) or transcript_obj
        cache["transcript"] = transcript_obj

        # 4. ç”Ÿæˆæ‘˜è¦åª’ä½“æ–‡ä»¶
        clip_paths, selected_frames, concat_video_path, gif_path = None, None, None, None
        if enable_summary_video or enable_gif:
            progress(0.7, desc="ç”Ÿæˆæ‘˜è¦åª’ä½“...")
            yield {run_status_html: update_status_and_sys_info("é˜¶æ®µ 1: ç”Ÿæˆæ‘˜è¦åª’ä½“...")}
            
            clip_paths, selected_frames, concat_video_path, gif_path = create_summary_media_artifacts(
                original_video_path=str(video_path),
                video_duration=cache["video_duration"],
                frames=cache["frames"],
                output_dir=video_output_dir,
                video_stem=video_path.stem,
                num_clips=summary_clips,
                clip_duration_around_keyframe=summary_duration,
                make_video=enable_summary_video,
                make_gif=enable_gif,
                gif_resolution=gif_resolution
            )
            cache["selected_summary_frames"] = selected_frames
            
            summary_clip_gallery_items = []
            if clip_paths and selected_frames:
                summary_clip_gallery_items = [
                    (path, f"ç‰‡æ®µä¸­å¿ƒ: {frame.timestamp:.2f}s")
                    for path, frame in zip(clip_paths, selected_frames)
                ]
                gr.Info(f"âœ… ç”Ÿæˆ {len(summary_clip_gallery_items)} ä¸ªæ‘˜è¦ç‰‡æ®µã€‚")

            gif_info_text = ""
            if gif_path and os.path.exists(gif_path):
                gif_size_kb = os.path.getsize(gif_path) / 1024
                gif_size_mb = gif_size_kb / 1024
                gif_info_text = f"**åŠ¨å›¾æ–‡ä»¶å¤§å°:** {gif_size_kb:.2f} KB ({gif_size_mb:.2f} MB)"
            
            yield {
                output_summary_clips_gallery: gr.update(value=summary_clip_gallery_items, visible=bool(summary_clip_gallery_items)),
                output_summary_video: gr.update(value=concat_video_path, visible=bool(concat_video_path)),
                output_gif: gr.update(value=gif_path, visible=bool(gif_path)),
                gif_info_md: gr.update(value=gif_info_text, visible=bool(gif_info_text)),
            }

        # 5. é˜¶æ®µä¸€å®Œæˆ
        analysis_state.status_message = "âœ… æ•°æ®æ•´ç†å®Œæˆï¼Œç‚¹å‡»ç»§ç»­ç”ŸæˆAIæ€»ç»“"
        progress(1.0)
       
        yield {
            run_status_html: update_status_and_sys_info(analysis_state.status_message),
            start_button: gr.update(visible=False),
            stop_button: gr.update(interactive=False),
            continue_button: gr.update(visible=True, interactive=True),
            analysis_cache_state: cache
        }
    except Exception as e:
        logger.error(f"æ•°æ®æå–é˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        analysis_state.status_message = f"âŒ é”™è¯¯: {e}"
        yield {
            run_status_html: update_status_and_sys_info(analysis_state.status_message),
            start_button: gr.update(interactive=True),
            stop_button: gr.update(interactive=False),
            analysis_cache_state: None
        }
    finally:
        analysis_state.is_running = False
        analysis_state.stop_requested = False

def phase_2_ai_analysis(
    cache: Dict, prompt_choice: str, custom_prompt: str, temperature: float, context_length: int,
    progress=gr.Progress(track_tqdm=True)
):
    if not app_state.is_loaded or not app_state.analyzer:
        gr.Error("æ¨¡å‹å°šæœªåŠ è½½ï¼")
        return
    if not cache:
        gr.Error("åˆ†æç¼“å­˜ä¸ºç©ºï¼Œè¯·å…ˆæ‰§è¡Œç¬¬ä¸€é˜¶æ®µçš„æ•°æ®æå–ï¼")
        return
        
    global analysis_state
    analysis_state = AnalysisState(is_running=True)
    
    logger.info("AIæ‘˜è¦ç”Ÿæˆå¼€å§‹")
    
    yield {
        run_status_html: update_status_and_sys_info("ğŸš€ é˜¶æ®µ 2: AIæ‘˜è¦ç”Ÿæˆå¼€å§‹..."),
        continue_button: gr.update(interactive=False),
        stop_button: gr.update(interactive=True),
        refresh_summary_button: gr.update(visible=False),
        output_report: "### ğŸ“œ AI æ‘˜è¦æŠ¥å‘Š\n\n"
    }
    
    app_state.analyzer.user_prompt = custom_prompt if prompt_choice == "è‡ªå®šä¹‰" else PRESET_PROMPTS[prompt_choice]
    app_state.analyzer.temperature = temperature
    app_state.analyzer.context_length = int(context_length)
    
    try:
        progress(0, desc="AIæ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
        current_report = "### ğŸ“œ AI æ‘˜è¦æŠ¥å‘Š\n\n"
        final_summary_text = ""
       
        stream = app_state.analyzer.summarize_all_frames_stream(cache["frames"], cache["transcript"])
        for chunk in stream:
            if analysis_state.stop_requested: raise InterruptedError("ç”¨æˆ·è¯·æ±‚åœæ­¢")
            if "__FULL_RESPONSE_END__" in chunk:
                final_summary_text = chunk.split("__FULL_RESPONSE_END__")[1]
                break
            current_report += chunk
            yield {output_report: current_report}
       
        if analysis_state.stop_requested: raise InterruptedError("ç”¨æˆ·è¯·æ±‚åœæ­¢")
        
        logger.info("AIæ‘˜è¦ç”Ÿæˆç»“æŸ")
        
        if MEDIAINFO_AVAILABLE:
            full_json = {"media_info": cache["media_info_json"], "ai_analysis": {"audio_transcript": cache["transcript"].text, "final_summary": final_summary_text}}
            yield {output_metadata_json: gr.update(value=full_json, visible=True)}
        
        analysis_state.status_message = "âœ… AIåˆ†æä»»åŠ¡å·²å®Œæˆï¼"
        progress(1.0)
        
        yield {
            run_status_html: update_status_and_sys_info(analysis_state.status_message),
            stop_button: gr.update(interactive=False),
            continue_button: gr.update(interactive=True),
            refresh_summary_button: gr.update(visible=True, interactive=True),
            analysis_cache_state: cache
        }
    except InterruptedError:
        analysis_state.status_message = "ğŸ›‘ åˆ†æå·²ç”±ç”¨æˆ·æ‰‹åŠ¨åœæ­¢ã€‚"
        logger.info(analysis_state.status_message)
        yield {
            run_status_html: update_status_and_sys_info(analysis_state.status_message),
            stop_button: gr.update(interactive=False),
            continue_button: gr.update(interactive=True),
            refresh_summary_button: gr.update(visible=True, interactive=True),
        }
    except Exception as e:
        logger.error(f"AIåˆ†æé˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        analysis_state.status_message = f"âŒ ä¸¥é‡é”™è¯¯: {e}"
        yield {
            run_status_html: update_status_and_sys_info(analysis_state.status_message),
            stop_button: gr.update(interactive=False),
            continue_button: gr.update(interactive=True),
        }
    finally:
        analysis_state.is_running = False
        analysis_state.stop_requested = False

def stop_analysis_func():
    if analysis_state.is_running:
        analysis_state.stop_requested = True
        logger.warning("æ”¶åˆ°åœæ­¢è¯·æ±‚ï¼Œå°†åœ¨å½“å‰æ­¥éª¤å®Œæˆåä¸­æ–­åˆ†æã€‚")
    return gr.update(interactive=False)

def save_settings(*args):
    keys = ["client_type", "ollama_model", "api_key", "api_url", "api_model", "prompt_choice", "custom_prompt", "temperature", "enable_audio", "frames_per_min", "max_frames", "context_length", "enable_summary_video", "enable_gif", "summary_clips", "summary_duration", "gif_resolution", "output_path"]
    settings = dict(zip(keys, args))
    with open(SETTINGS_FILE, "w", encoding="utf-8") as f: json.dump(settings, f, indent=2)
    gr.Info("âœ… è®¾ç½®å·²ä¿å­˜ï¼")

def load_settings_and_refresh_models():
    ollama_models = detect_ollama_models()
    default_settings = ["Ollama", gr.update(choices=ollama_models, value=ollama_models[0] if ollama_models else None), "", "http://localhost:1234/v1", "", "å†…å®¹æ€»ç»“ä¸è¯„ä¼°", "", 0.5, True, 30, 25, 4096, True, False, 10, 5.0, "ä¸­", "gradio_output"]
    if not SETTINGS_FILE.exists(): return default_settings
    try:
        with open(SETTINGS_FILE, "r", encoding="utf-8") as f: settings = json.load(f)
        saved_model = settings.get("ollama_model", "")
        selected_model = saved_model if saved_model in ollama_models else (ollama_models[0] if ollama_models else None)
        return [
            settings.get("client_type", "Ollama"),
            gr.update(choices=ollama_models, value=selected_model),
            settings.get("api_key", ""),
            settings.get("api_url", "http://localhost:1234/v1"),
            settings.get("api_model", ""),
            settings.get("prompt_choice", "å†…å®¹æ€»ç»“ä¸è¯„ä¼°"),
            settings.get("custom_prompt", ""),
            settings.get("temperature", 0.5),
            settings.get("enable_audio", True),
            settings.get("frames_per_min", 30),
            settings.get("max_frames", 25),
            settings.get("context_length", 4096),
            settings.get("enable_summary_video", True),
            settings.get("enable_gif", False),
            settings.get("summary_clips", 10),
            settings.get("summary_duration", 5.0),
            settings.get("gif_resolution", "ä¸­"),
            settings.get("output_path", "gradio_output")
        ]
    except (json.JSONDecodeError, KeyError):
        logger.warning("æ— æ³•è§£æè®¾ç½®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤è®¾ç½®ã€‚")
        return default_settings

def show_frame_details(cache: Dict, evt: gr.SelectData):
    if not cache or not cache.get("frames"):
        return gr.update(visible=False), ""
   
    try:
        selected_frame: Frame = cache["frames"][evt.index]
        metrics = selected_frame.metrics
       
        md_text = f"#### ğŸ–¼ï¸ å¸§è¯¦æƒ… (æ—¶é—´: {selected_frame.timestamp:.2f}s)\n\n"
        md_text += "| å‚æ•° | å€¼ |\n|---|---|\n"
        for key, value in metrics.items():
            md_text += f"| {key} | {value:.2f} |\n"
           
        return gr.update(visible=True), md_text
    except (IndexError, KeyError) as e:
        logger.warning(f"æ— æ³•æ˜¾ç¤ºå¸§è¯¦æƒ…: {e}")
        return gr.update(visible=False), "æ— æ³•åŠ è½½è¯¥å¸§çš„è¯¦ç»†æ•°æ®ã€‚"

def show_clip_details(cache: Dict, evt: gr.SelectData):
    if not cache or not cache.get("selected_summary_frames"):
        return gr.update(visible=False), ""
   
    try:
        selected_frame: Frame = cache["selected_summary_frames"][evt.index]
        metrics = selected_frame.metrics
       
        md_text = f"#### ğŸ¬ ç‰‡æ®µä¸­å¿ƒå¸§è¯¦æƒ… (æ—¶é—´: {selected_frame.timestamp:.2f}s)\n\n"
        md_text += "æ­¤ç‰‡æ®µæ˜¯å›´ç»•è¯¥æ—¶é—´ç‚¹çš„å…³é”®å¸§ç”Ÿæˆçš„ã€‚\n\n"
        md_text += "| å‚æ•° | å€¼ |\n|---|---|\n"
        for key, value in metrics.items():
            md_text += f"| {key} | {value:.2f} |\n"
           
        logger.info(f"æ˜¾ç¤ºç‰‡æ®µè¯¦æƒ…: æ—¶é—´ {selected_frame.timestamp:.2f}s")
        return gr.update(visible=True), md_text
    except (IndexError, KeyError) as e:
        logger.warning(f"æ— æ³•æ˜¾ç¤ºç‰‡æ®µè¯¦æƒ…: {e}")
        return gr.update(visible=False), "æ— æ³•åŠ è½½è¯¥ç‰‡æ®µçš„è¯¦ç»†æ•°æ®ã€‚"

# ==============================================================================
# é˜¶æ®µä¸‰ï¼šUIå®šä¹‰ä¸å¯åŠ¨åŒº
# ==============================================================================
CSS = """.stats-container { display: flex; flex-wrap: wrap; justify-content: space-between; gap: 10px; font-size: 0.9em; } .stat-item { flex: 1; min-width: 120px; background-color: #f0f0f0; border-radius: 5px; padding: 5px; } .label { font-weight: bold; } .value { float: right; } .bar-container { width: 100%; background-color: #e0e0e0; border-radius: 3px; height: 8px; margin-top: 3px; } .bar { height: 100%; border-radius: 3px; } .cpu { background-color: #4CAF50; } .ram { background-color: #2196F3; } .gpu { background-color: #ff9800; } .vram { background-color: #f44336; } footer { display: none !important; }"""

def create_ui():
    global status_box, client_type_dd, model_select, api_key_txt, api_url_txt, api_model_txt, load_button, unload_button
    global output_report, output_metadata_table, metadata_plot, output_gallery, output_summary_video, output_gif, output_metadata_json
    global output_summary_clips_gallery, clip_details_accordion, clip_details_md
    global run_status_html, analysis_progress, start_button, continue_button, stop_button, refresh_summary_button, clear_outputs_button
    global frame_details_accordion, frame_details_md, analysis_cache_state, gif_info_md
   
    with gr.Blocks(css=CSS, title="è§†é¢‘æ·±åº¦åˆ†æå¹³å°", theme=gr.themes.Soft()) as iface:
        analysis_cache_state = gr.State(None)
        gr.Markdown("# ğŸš€ è§†é¢‘æ·±åº¦åˆ†æå¹³å° (V3.5 ç¨³å®šç‰ˆ)")
        if not FONT_LOADED_SUCCESSFULLY:
            gr.Markdown("<div style='background-color: #FFDDDD; color: #D8000C; padding: 10px; border-radius: 5px;'>âš ï¸ **ä¸¥é‡è­¦å‘Š**: æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆçš„ä¸­æ–‡å­—ä½“ã€‚å›¾è¡¨ä¸­çš„ä¸­æ–‡å°†æ˜¾ç¤ºä¸ºæ–¹æ¡†ã€‚</div>")
        if not FFMPEG_AVAILABLE:
            gr.Markdown("<div style='background-color: #FFDDDD; color: #D8000C; padding: 10px; border-radius: 5px;'>âš ï¸ **FFmpeg è­¦å‘Š**: æœªæ£€æµ‹åˆ° FFmpegã€‚AIæ‘˜è¦è§†é¢‘/GIF å°†æ— æ³•ç”Ÿæˆã€‚è¯·å®‰è£… FFmpegã€‚</div>")
       
        with gr.Row():
            with gr.Column(scale=1):
                with gr.Accordion("1. æ¨¡å‹é…ç½®", open=True):
                    status_box = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", value="æœªåŠ è½½", interactive=False)
                    client_type_dd = gr.Dropdown(["Ollama", "OpenAI-compatible API"], label="å®¢æˆ·ç«¯ç±»å‹", value="Ollama")
                    with gr.Group(visible=True) as ollama_group:
                        model_select = gr.Dropdown(label="é€‰æ‹©Ollamaæ¨¡å‹", interactive=True, info="è¯·ç¡®ä¿é€‰æ‹©çš„æ˜¯å¤šæ¨¡æ€(VL)æ¨¡å‹ï¼Œå¦‚qwen-vl, llavaç­‰")
                        refresh_button = gr.Button("ğŸ”„ åˆ·æ–°å¯ç”¨æ¨¡å‹")
                    with gr.Group(visible=False) as api_group:
                        api_url_txt = gr.Textbox(label="API URL (LM Studio / OpenAI)", value="http://localhost:1234/v1", placeholder="ä¾‹å¦‚: http://localhost:1234/v1")
                        api_key_txt = gr.Textbox(label="API Key (å¯é€‰)", type="password", placeholder="æœ¬åœ°æœåŠ¡é€šå¸¸æ— éœ€å¡«å†™")
                        api_model_txt = gr.Textbox(label="æ¨¡å‹åç§°", placeholder="ä¾‹å¦‚: åœ¨LM Studioä¸­åŠ è½½çš„æ¨¡å‹ID")
                    load_button = gr.Button("âœ… åŠ è½½æ¨¡å‹", variant="primary")
                    unload_button = gr.Button("å¸è½½æ¨¡å‹", interactive=False)
               
                with gr.Accordion("Ollama çŠ¶æ€ç›‘æ§ä¸ç®¡ç†", open=False):
                    ollama_status_markdown = gr.Markdown("æ­£åœ¨è·å–çŠ¶æ€...")
                    running_models_df = gr.DataFrame(headers=["è¿è¡Œä¸­çš„æ¨¡å‹", "å ç”¨å†…å­˜"], interactive=False, row_count=(0, "dynamic"))
                    with gr.Row():
                        unload_model_dd = gr.Dropdown(label="é€‰æ‹©è¦å¸è½½çš„æ¨¡å‹", interactive=True)
                        unload_model_button = gr.Button("âš¡ å¸è½½é€‰ä¸­æ¨¡å‹")
                    refresh_status_button = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", elem_id="refresh_ollama_status_button")
               
                with gr.Accordion("2. ä¸Šä¼ ä¸åˆ†æè®¾ç½®", open=True):
                    file_output = gr.File(label="å¾…åˆ†æè§†é¢‘", file_count="single", interactive=True, file_types=["video"])
                    upload_button = gr.UploadButton("ğŸ“ ç‚¹å‡»æˆ–æ‹–æ‹½å•ä¸ªè§†é¢‘ä¸Šä¼ ", file_count="single", file_types=["video"])
                    prompt_choice_dd = gr.Dropdown(label="é€‰æ‹©æç¤ºè¯æ¨¡æ¿", choices=list(PRESET_PROMPTS.keys()))
                    custom_prompt_txt = gr.Textbox(label="è‡ªå®šä¹‰æç¤ºè¯", lines=3, visible=False)
                    temp_slider = gr.Slider(0.0, 1.5, step=0.1, label="æ¸©åº¦ (Temperature)")
                    output_path_txt = gr.Textbox(label="åˆ†æç»“æœè¾“å‡ºè·¯å¾„", value="gradio_output")
               
                with gr.Accordion("3. é«˜çº§å‚æ•°ä¸ç»´æŠ¤", open=False):
                    frames_per_min_slider = gr.Slider(1, 120, step=1, label="æ¯åˆ†é’Ÿå…³é”®å¸§æ•°")
                    max_frames_slider = gr.Slider(5, 100, step=1, label="æœ€å¤§æ€»å¸§æ•°")
                    context_length_slider = gr.Slider(1024, 16384, step=256, label="æ¨¡å‹ä¸Šä¸‹æ–‡ (Context) é•¿åº¦", value=4096)
                    detect_context_button = gr.Button("ğŸ” æ£€æµ‹å¹¶è®¾ç½®æ¨èä¸Šä¸‹æ–‡")
                    enable_audio_checkbox = gr.Checkbox(label="å¯ç”¨éŸ³é¢‘è½¬å½•")
                    with gr.Group():
                        gr.Markdown("#### AIæ‘˜è¦ä¸GIFç”Ÿæˆ")
                        enable_summary_video_cb = gr.Checkbox(label="ç”ŸæˆAIæ‘˜è¦åª’ä½“", value=True)
                        enable_gif_cb = gr.Checkbox(label="ç”ŸæˆGIFåŠ¨å›¾")
                        summary_clips_slider = gr.Slider(3, 30, step=1, label="æ‘˜è¦ç‰‡æ®µæ•°é‡")
                        summary_duration_slider = gr.Slider(1.0, 10.0, step=0.5, label="æ¯ä¸ªç‰‡æ®µæ€»æ—¶é•¿(ç§’)")
                        gif_resolution_dd = gr.Dropdown(["ä½", "ä¸­", "é«˜"], label="GIFåˆ†è¾¨ç‡", value="ä¸­")
                    with gr.Row():
                        save_settings_button = gr.Button("ğŸ’¾ ä¿å­˜æ‰€æœ‰è®¾ç½®")
                        clear_outputs_button = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰è¾“å‡º", variant="stop")
               
                with gr.Blocks():
                    start_button = gr.Button("1. å¼€å§‹æå–æ•°æ®", variant="primary", size='lg')
                    continue_button = gr.Button("2. ç»§ç»­ç”ŸæˆAIæ€»ç»“", variant="primary", size='lg', visible=False)
                    with gr.Row():
                        stop_button = gr.Button("ğŸ›‘ åœæ­¢", variant="stop", interactive=False, scale=1)
                        refresh_summary_button = gr.Button("ğŸ”„ ä»…åˆ·æ–°AIæ€»ç»“", interactive=True, visible=False, scale=1)
            with gr.Column(scale=2):
                run_status_html = gr.HTML(update_status_and_sys_info())
                analysis_progress = gr.Progress()
                with gr.Tabs():
                    with gr.TabItem("ğŸ“ AI æ‘˜è¦æŠ¥å‘Š"):
                        output_report = gr.Markdown()
                    with gr.TabItem("ğŸ¬ æ‘˜è¦åª’ä½“"):
                        gr.Markdown("#### è§†é¢‘ç‰‡æ®µæ‘˜è¦ (å¯ç‚¹å‡»æ’­æ”¾)\nç‚¹å‡»ä¸‹æ–¹çš„è§†é¢‘ç‰‡æ®µä»¥æŸ¥çœ‹å…¶ä¸­å¿ƒå…³é”®å¸§çš„è¯¦ç»†æŠ€æœ¯æŒ‡æ ‡ã€‚")
                        output_summary_clips_gallery = gr.Gallery(label="è§†é¢‘ç‰‡æ®µæ‘˜è¦", columns=4, height="auto", object_fit="contain", visible=False, allow_preview=True)
                        with gr.Accordion("ç‰‡æ®µè¯¦æƒ…", open=False, visible=False) as clip_details_accordion:
                            clip_details_md = gr.Markdown()
                        gr.Markdown("---")
                        with gr.Row():
                            with gr.Column():
                                output_summary_video = gr.Video(label="å®Œæ•´æ‘˜è¦è§†é¢‘ (æ‹¼æ¥ç‰ˆ)", visible=False)
                            with gr.Column():
                                output_gif = gr.Image(label="æ‘˜è¦GIFåŠ¨å›¾", type="filepath", visible=False)
                                gif_info_md = gr.Markdown(visible=False)
                    with gr.TabItem("ğŸ–¼ï¸ å…³é”®å¸§ç”»å»Š"):
                        output_gallery = gr.Gallery(label="å…³é”®å¸§", columns=6, height="auto", object_fit="contain", visible=False)
                        with gr.Accordion("å•å¸§è¯¦æƒ…", open=False, visible=False) as frame_details_accordion:
                            frame_details_md = gr.Markdown()
                    with gr.TabItem("ğŸ“Š å…ƒæ•°æ®ä¸ç”»è´¨"):
                        output_metadata_table = gr.Markdown(visible=False)
                        metadata_plot = gr.Plot(label="ç”»è´¨åˆ†æå›¾", visible=False)
                    with gr.TabItem("ğŸ“„ è¯¦ç»†å…ƒæ•°æ® (JSON)"):
                        output_metadata_json = gr.JSON(label="å¯äº¤äº’çš„å…ƒæ•°æ®æ ‘çŠ¶å›¾", visible=False)
       
        all_settings = [client_type_dd, model_select, api_key_txt, api_url_txt, api_model_txt, prompt_choice_dd, custom_prompt_txt, temp_slider, enable_audio_checkbox, frames_per_min_slider, max_frames_slider, context_length_slider, enable_summary_video_cb, enable_gif_cb, summary_clips_slider, summary_duration_slider, gif_resolution_dd, output_path_txt]
       
        # --- äº‹ä»¶ç»‘å®š ---
        client_type_dd.change(lambda x: (gr.update(visible=x=="Ollama"), gr.update(visible=x!="Ollama")), client_type_dd, [ollama_group, api_group])
        prompt_choice_dd.change(lambda x: gr.update(visible=x=="è‡ªå®šä¹‰"), prompt_choice_dd, custom_prompt_txt)
       
        refresh_button.click(refresh_models_action, outputs=model_select)
        load_button.click(load_model_action, [client_type_dd, model_select, api_key_txt, api_url_txt, api_model_txt], [status_box, load_button, unload_button])
        unload_button.click(unload_model_action, outputs=[status_box, load_button, unload_button])
       
        save_settings_button.click(save_settings, all_settings)
       
        upload_button.upload(lambda file: file.name if file else None, inputs=[upload_button], outputs=[file_output])
       
        clear_outputs_button.click(
            clear_all_outputs_action,
            outputs=[
                run_status_html, output_report, output_metadata_table, metadata_plot,
                output_gallery, output_summary_video, output_gif, gif_info_md, output_metadata_json,
                output_summary_clips_gallery, clip_details_accordion, clip_details_md,
                frame_details_accordion, frame_details_md, analysis_cache_state,
                start_button, continue_button, stop_button, refresh_summary_button
            ]
        )
        
        phase1_inputs = [
            file_output, enable_audio_checkbox, frames_per_min_slider, max_frames_slider, output_path_txt,
            enable_summary_video_cb, enable_gif_cb, summary_clips_slider, summary_duration_slider, gif_resolution_dd
        ]
        phase1_outputs = [
            run_status_html, start_button, stop_button, continue_button, refresh_summary_button,
            output_report, output_metadata_table, metadata_plot, output_gallery,
            output_metadata_json, output_summary_video, output_gif, gif_info_md, frame_details_accordion,
            output_summary_clips_gallery, clip_details_accordion,
            analysis_cache_state
        ]
        start_button.click(phase_1_extraction, phase1_inputs, phase1_outputs)
        
        phase2_inputs = [
            analysis_cache_state, prompt_choice_dd, custom_prompt_txt, temp_slider, context_length_slider
        ]
        phase2_outputs = [
            run_status_html, continue_button, stop_button, refresh_summary_button, output_report,
            output_metadata_json, analysis_cache_state
        ]
        continue_button.click(phase_2_ai_analysis, phase2_inputs, phase2_outputs)
       
        refresh_summary_button.click(phase_2_ai_analysis, phase2_inputs, phase2_outputs)
        stop_button.click(stop_analysis_func, outputs=[stop_button])
       
        output_gallery.select(show_frame_details, [analysis_cache_state], [frame_details_accordion, frame_details_md])
        output_summary_clips_gallery.select(show_clip_details, [analysis_cache_state], [clip_details_accordion, clip_details_md])
       
        ollama_status_outputs = [ollama_status_markdown, running_models_df, unload_model_dd]
        refresh_status_button.click(get_ollama_status, outputs=ollama_status_outputs)
        unload_model_button.click(unload_ollama_model, inputs=[unload_model_dd], outputs=ollama_status_outputs)
        detect_context_button.click(detect_and_set_context, inputs=[model_select], outputs=[context_length_slider])
       
        iface.load(load_settings_and_refresh_models, outputs=all_settings)
        iface.load(get_ollama_status, outputs=ollama_status_outputs)
       
    return iface

if __name__ == "__main__":
    prompt_dir = Path("prompts/frame_analysis")
    prompt_dir.mkdir(parents=True, exist_ok=True)
   
    summary_prompt_path = prompt_dir / "video_summary.txt"
    summary_prompt_content = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚æ¥ä¸‹æ¥æˆ‘ä¼šç»™ä½ ä¸€ä¸ªè§†é¢‘çš„å¤šä¸ªå…³é”®å¸§å›¾åƒï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰ã€ä»¥åŠå¯é€‰çš„éŸ³é¢‘è½¬å½•å†…å®¹ã€‚\n\n"
        "ç”¨æˆ·çš„æ ¸å¿ƒåˆ†æè¦æ±‚æ˜¯ï¼š{user_prompt}\n\n"
        "---éŸ³é¢‘è½¬å½•---\n{audio_transcript}\n\n"
        "---å…³é”®å¸§æ—¶é—´ç‚¹åˆ—è¡¨---\n{frame_info}\n\n"
        "è¯·ç»¼åˆä½ çœ‹åˆ°çš„æ‰€æœ‰å›¾åƒå’Œå¬åˆ°çš„æ‰€æœ‰æ–‡æœ¬ï¼Œç”Ÿæˆä¸€ä»½å…¨é¢ã€æµç•…ã€ç»“æ„åŒ–çš„è§†é¢‘æœ€ç»ˆåˆ†ææŠ¥å‘Šã€‚æŠ¥å‘Šéœ€è¦ç›´æ¥å›åº”ç”¨æˆ·çš„æ ¸å¿ƒè¦æ±‚ã€‚\n"
        "é‡è¦æŒ‡ä»¤ï¼šè¯·å°†æ‰€æœ‰å›¾åƒè§†ä¸ºä¸€ä¸ªæ•´ä½“æ•…äº‹çº¿ï¼Œè¿›è¡Œè¿è´¯çš„å™è¿°å’Œåˆ†æï¼Œè€Œä¸æ˜¯å­¤ç«‹åœ°æè¿°æ¯ä¸€å¼ å›¾ã€‚å¦‚æœå¤šä¸ªå…³é”®å¸§å†…å®¹ç›¸ä¼¼ï¼Œè¯·è¿›è¡Œæ¦‚æ‹¬æ€§æè¿°ï¼Œé¿å…é‡å¤ã€‚\n"
        "**è¾“å‡ºè¦æ±‚**ï¼šè¯·ç›´æ¥è¾“å‡ºMarkdownæ ¼å¼çš„æŠ¥å‘Šå…¨æ–‡ã€‚**ä¸¥ç¦**åœ¨æŠ¥å‘Šä¸­åŠ å…¥ä»»ä½•å¯¹è¯æ€§æ–‡å­—ã€æé—®ï¼ˆä¾‹å¦‚ä¸è¦è¯´â€˜ä½ å¯¹è¿™ä¸ªåˆ†ææ»¡æ„å—ï¼Ÿâ€™æˆ–â€˜ä½ çš„åé¦ˆå°†å¸®åŠ©æˆ‘...â€™ï¼‰ã€æˆ–å›¾åƒå ä½ç¬¦ï¼ˆå¦‚`[img-n]`ï¼‰ã€‚ä½ çš„å›ç­”åº”è¯¥**ä»…é™äº**æŠ¥å‘Šæœ¬èº«ï¼Œå†…å®¹ç¿”å®ï¼Œç»“æ„æ¸…æ™°ã€‚"
    )
    with open(summary_prompt_path, "w", encoding="utf-8") as f:
        f.write(summary_prompt_content)
    logger.info(f"å·²æ›´æ–°/åˆ›å»ºä¼˜åŒ–åçš„æ‘˜è¦æç¤ºè¯æ–‡ä»¶: {summary_prompt_path}")
    
    app_state.stop_monitoring.clear()
    monitor_thread = threading.Thread(target=monitor_system_stats, daemon=True)
    monitor_thread.start()
   
    iface = create_ui()
   
    try:
        logger.info("æ­£åœ¨å¯åŠ¨ Gradio Web å¹³å°...")
        logger.info("è„šæœ¬å°†è‡ªåŠ¨æ‰“å¼€æ‚¨çš„æµè§ˆå™¨ã€‚")
        logger.info("æ‚¨ä¹Ÿå¯ä»¥åœ¨æœ¬åœ°é€šè¿‡ http://0.0.0.0:8001 è®¿é—®ã€‚")
        iface.queue().launch(server_name="0.0.0.0", server_port=8001, debug=False, inbrowser=True)
    except (KeyboardInterrupt, OSError):
        logger.info("\næ­£åœ¨å¹³ç¨³å…³é—­ï¼Œè¯·ç¨å€™...")
    finally:
        analysis_state.stop_requested = True
        app_state.stop_monitoring.set()
        if NVIDIA_GPU_AVAILABLE:
            try:
                pynvml.nvmlShutdown()
                logger.info("pynvml å·²æˆåŠŸå…³é—­ã€‚")
            except: pass
        logger.info("åº”ç”¨ç¨‹åºå·²å…³é—­ã€‚")
